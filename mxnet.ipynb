{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mxnet](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mxnet_logo_2.png)\n",
    "\n",
    "## What is MXNet ?\n",
    "\n",
    "Build to ease the development of deep learning algorithms, MXNet is a powerful open-source deep learning framework instrument. In the last few years, the impact of deep learning has been widespread from healthcare to transportation to manufacturing and more. Deep learning is sought by companies to solve hard problems like speech recognition, object recognition and machine translation.\n",
    "MXNet is used to define, train and deploy deep neural networks. It is lean, flexible and ultra-scalable i.e. it allows fast model-training and supports a flexible programming model and multiple languages.\n",
    "\n",
    "## Why MXNet ?\n",
    "\n",
    "The problem with existing deep learning frameworks is that the users need to learn another system for a different programming flavor. MXNet solves this issue.\n",
    "A successful deep learning framework is one which excels in Programmability, Portability, and Scalability.\n",
    "MXNet supports multiple languages like C++, Python, R, Julia, Perl etc. This eliminates the need for learning a new language to use the framework and simplify network definitions. MXNet models are portable in a manner that they are able to fit in very small amounts of memory. Hence one can train his/her model in the cloud and deploy iton either mobile or connected device. MXNet can also scale to multiple GPUs and multiple machines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of Mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from mxnet) (2.24.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from mxnet) (1.19.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details installation can be found here\n",
    "\n",
    "[pip install keras-tuner](https://mxnet.apache.org/versions/1.2.1/install/index.html?platform=Linux&language=Python&processor=CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars\n",
    "\n",
    "If you never studied linear algebra or machine learning, \n",
    "you're probably used to working with one number at a time.\n",
    "And know how to do basic things like add them together or multiply them.\n",
    "For example, in Palo Alto, the temperature is $52$ degrees Fahrenheit. \n",
    "Formally, we call these values $scalars$.\n",
    "If you wanted to convert this value to Celsius (using metric system's more sensible unit of temperature measurement),\n",
    "you'd evaluate the expression $c = (f - 32) * 5/9$ setting $f$ to $52$.\n",
    "In this equation, each of the terms $32$, $5$, and $9$ is a scalar value.\n",
    "The placeholders $c$ and $f$ that we use are called variables\n",
    "and they stand in for unknown scalar values.\n",
    "\n",
    "In mathematical notation, we represent scalars with ordinary lower cased letters ($x$, $y$, $z$).\n",
    "We also denote the space of all scalars as $\\mathcal{R}$.\n",
    "For expedience, we're going to punt a bit on what precisely a space is,\n",
    "but for now, remember that if you want to say that $x$ is a scalar, \n",
    "you can simply say $x \\in \\mathcal{R}$.\n",
    "The symbol $\\in$ can be pronounced \"in\" and just denotes membership in a set.\n",
    "\n",
    "In MXNet, we work with scalars by creating NDArrays with just one element. \n",
    "In this snippet, we instantiate two scalars and perform some familiar arithmetic operations with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y =  \n",
      "[5.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "x * y =  \n",
      "[6.]\n",
      "<NDArray 1 @cpu(0)>\n",
      "x / y =  \n",
      "[1.5]\n",
      "<NDArray 1 @cpu(0)>\n",
      "x ** y =  \n",
      "[9.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# Instantiate two scalars\n",
    "##########################\n",
    "x = nd.array([3.0]) \n",
    "y = nd.array([2.0])\n",
    "\n",
    "##########################\n",
    "# Add them\n",
    "##########################\n",
    "print('x + y = ', x + y)\n",
    "\n",
    "##########################\n",
    "# Multiply them\n",
    "##########################\n",
    "print('x * y = ', x * y)\n",
    "\n",
    "##########################\n",
    "# Divide x by y\n",
    "##########################\n",
    "print('x / y = ', x / y)\n",
    "\n",
    "##########################\n",
    "# Raise x to the power y. \n",
    "##########################\n",
    "print('x ** y = ', nd.power(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert any NDArray to a Python float by calling its `asscalar` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors \n",
    "You can think of a vector as simply a list of numbers, for example ``[1.0,3.0,4.0,2.0]``. \n",
    "Each of the numbers in the vector consists of a single scalar value.\n",
    "We call these values the *entries* or *components* of the vector.\n",
    "Often, we're interested in vectors whose values hold some real-world significance.\n",
    "For example, if we're studying the risk that loans default,\n",
    "we might associate each applicant with a vector \n",
    "whose components correspond to their income, \n",
    "length of employment, number of previous defaults, etc. \n",
    "If we were studying the risk of heart attack in hospital patients, \n",
    "we might represent each patient with a vector\n",
    "whose components capture their most recent vital signs,\n",
    "cholesterol levels, minutes of exercise per day, etc. \n",
    "In math notation, we'll usually denote vectors as bold-faced, \n",
    "lower-cased letters ($\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{w})$. \n",
    "In MXNet, we work with vectors via 1D NDArrays with an arbitrary number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u =  \n",
      "[0. 1. 2. 3.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "u = nd.arange(4)\n",
    "print('u = ', u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to any element of a vector by using a subscript. \n",
    "For example, we can refer to the $4$th element of $\\mathbf{u}$ by $u_4$. \n",
    "Note that the element $u_4$ is a scalar, \n",
    "so we don't bold-face the font when referring to it.\n",
    "In code, we access any element $i$ by indexing into the ``NDArray``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[3.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length, dimensionality, and, shape\n",
    "\n",
    "A vector is just an array of numbers. And just as every array has a length, so does every vector. \n",
    "In math notation, if we want to say that a vector $x$ consists of $n$ real-valued scalars,\n",
    "we can express this as $\\mathbf{x} \\in \\mathcal{R}^n$.\n",
    "The length of a vector is commonly called its $dimension$.\n",
    "As with an ordinary Python array, we can access the length of an NDArray \n",
    "by calling Python's in-built ``len()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access a vector's length via its `.shape` attribute. \n",
    "The shape is a tuple that lists the dimensionality of the NDArray along each of its axes. \n",
    "Because a vector can only be indexed along one axis, its shape has just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the word dimension is overloaded and this tends to confuse people.\n",
    "Some use the *dimensionality* of a vector to refer to its length (the number of components). \n",
    "However some use the word *dimensionality* to refer to the number of axes that an array has.\n",
    "In this sense, a scalar *would have* $0$ dimensions and a vector *would have* $1$ dimension.\n",
    "**To avoid confusion, when we say *2D* array or *3D* array, we mean an array with 2 or 3 axes repespectively. But if we say *$n$-dimensional* vector, we mean a vector of length $n$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2. 4. 6.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "\n",
      "[12. 24. 36.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = nd.array([1,2,3])\n",
    "y = nd.array([10,20,30])\n",
    "print(a * x)\n",
    "print(a * x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "Just as vectors generalize scalars from order $0$ to order $1$, \n",
    "matrices generalize vectors from $1D$ to $2D$. \n",
    "Matrices, which we'll denote with capital letters ($A$, $B$, $C$), \n",
    "are represented in code as arrays with 2 axes.  \n",
    "Visually, we can draw a matrix as a table,\n",
    "where each entry $a_{ij}$ belongs to the $i$-th row and $j$-th column. \n",
    "\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "We can create a matrix with $n$ rows and $m$ columns in MXNet\n",
    "by specifying a shape with two components `(n,m)`\n",
    "when calling any of our favorite functions for instantiating an `ndarray`\n",
    "such as `ones`, or `zeros`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.zeros((5,4))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reshape any 1D array into a 2D ndarray by calling `ndarray`'s reshape method and passing in the desired shape. Note that the product of shape components `n * m` must be equal to the length of the original vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.]\n",
       " [ 4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11.]\n",
       " [12. 13. 14. 15.]\n",
       " [16. 17. 18. 19.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.arange(20)\n",
    "A = x.reshape((5, 4))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are useful data structures: they allow us to organize data that has different modalities of variation. For example, returning to the example of medical data, rows in our matrix might correspond to different patients, while columns might correspond to different attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the scalar elements $a_{ij}$ of a matrix $A$ by specifying the indices for the row ($i$) and column ($j$) respectively. Let's grab the element $a_{2,3}$ from the random matrix we initialized above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A[2, 3] =  \n",
      "[11.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('A[2, 3] = ', A[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also grab the vectors corresponding to an entire row $\\mathbf{a}_{i,:}$ or a column $\\mathbf{a}_{:,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row 2 \n",
      "[ 8.  9. 10. 11.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "column 3 \n",
      "[ 3.  7. 11. 15. 19.]\n",
      "<NDArray 5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('row 2', A[2, :])\n",
    "print('column 3', A[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transpose the matrix through `T`. That is, if $B = A^T$, then $b_{ij} = a_{ji}$ for any $i$ and $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  4.  8. 12. 16.]\n",
       " [ 1.  5.  9. 13. 17.]\n",
       " [ 2.  6. 10. 14. 18.]\n",
       " [ 3.  7. 11. 15. 19.]]\n",
       "<NDArray 4x5 @cpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors \n",
    "\n",
    "Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures with even more axes. Tensors give us a generic way of discussing arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors.\n",
    "\n",
    "Using tensors will become more important when we start working with images, which arrive as 3D data structures, with axes corresponding to the height, width, and the three (RGB) color channels. But in this chapter, we're going to skip past and make sure you know the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (2, 3, 4)\n",
      "X = \n",
      "[[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]]\n",
      "\n",
      " [[12. 13. 14. 15.]\n",
      "  [16. 17. 18. 19.]\n",
      "  [20. 21. 22. 23.]]]\n",
      "<NDArray 2x3x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "X = nd.arange(24).reshape((2, 3, 4))\n",
    "print('X.shape =', X.shape)\n",
    "print('X =', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise operations\n",
    "\n",
    "Oftentimes, we want to apply functions to arrays. \n",
    "Some of the simplest and most useful functions are the element-wise functions. \n",
    "These operate by performing a single scalar operation on the corresponding elements of two arrays.\n",
    "We can create an element-wise function from any function that maps from the scalars to the scalars.\n",
    "In math notations we would denote such a function as $f: \\mathcal{R} \\rightarrow \\mathcal{R}$.\n",
    "Given any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*, and the function f,\n",
    "we can produce a vector $\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$ \n",
    "by setting $c_i \\gets f(u_i, v_i)$ for all $i$.\n",
    "Here, we produced the vector-valued $F: \\mathcal{R}^d \\rightarrow \\mathcal{R}^d$\n",
    "by *lifting* the scalar function to an element-wise vector operation.\n",
    "In MXNet, the common standard arithmetic operators (+,-,/,\\*,\\*\\*)\n",
    "have all been *lifted* to element-wise operations for identically-shaped tensors of arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = \n",
      "[2. 2. 2. 2.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u + v \n",
      "[ 3.  4.  6. 10.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u - v \n",
      "[-1.  0.  2.  6.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u * v \n",
      "[ 2.  4.  8. 16.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "u / v \n",
      "[0.5 1.  2.  4. ]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "u = nd.array([1, 2, 4, 8])\n",
    "v = nd.ones_like(u) * 2\n",
    "print('v =', v)\n",
    "print('u + v', u + v)\n",
    "print('u - v', u - v)\n",
    "print('u * v', u * v)\n",
    "print('u / v', u / v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call element-wise operations on any two tensors of the same shape, including matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B = \n",
      "[[3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]]\n",
      "<NDArray 5x4 @cpu(0)>\n",
      "A + B = \n",
      "[[ 3.  4.  5.  6.]\n",
      " [ 7.  8.  9. 10.]\n",
      " [11. 12. 13. 14.]\n",
      " [15. 16. 17. 18.]\n",
      " [19. 20. 21. 22.]]\n",
      "<NDArray 5x4 @cpu(0)>\n",
      "A * B = \n",
      "[[ 0.  3.  6.  9.]\n",
      " [12. 15. 18. 21.]\n",
      " [24. 27. 30. 33.]\n",
      " [36. 39. 42. 45.]\n",
      " [48. 51. 54. 57.]]\n",
      "<NDArray 5x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "B = nd.ones_like(A) * 3\n",
    "print('B =', B)\n",
    "print('A + B =', A + B)\n",
    "print('A * B =', A * B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic properties of tensor arithmetic\n",
    "\n",
    "Scalars, vectors, matrices, and tensors of any order have some nice properties that we'll often rely on.\n",
    "For example, as you might have noticed from the definition of an element-wise operation, \n",
    "given operands with the same shape, \n",
    "the result of any element-wise operation is a tensor of that same shape. \n",
    "Another convenient property is that for all tensors, multiplication by a scalar \n",
    "produces a tensor of the same shape. \n",
    "In math, given two tensors $X$ and $Y$ with the same shape,\n",
    "$\\alpha X + Y$ has the same shape. \n",
    "(numerical mathematicians call this the AXPY operation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3,)\n",
      "(3,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = nd.ones(3)\n",
    "y = nd.zeros(3)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print((a * x).shape)\n",
    "print((a * x + y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape is not the the only property preserved under addition and multiplication by a scalar. These operations also preserve membership in a vector space. But we'll postpone this discussion for the second half of this chapter because it's not critical to getting your first models up and running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sums and means \n",
    "\n",
    "The next more sophisticated thing we can do with arbitrary tensors \n",
    "is to calculate the sum of their elements. \n",
    "In mathematical notation, we express sums using the $\\sum$ symbol. \n",
    "To express the sum of the elements in a vector $\\mathbf{u}$ of length $d$, \n",
    "we can write $\\sum_{i=1}^d u_i$. In code, we can just call ``nd.sum()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[15.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.sum(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly express sums over the elements of tensors of arbitrary shape. For example, the sum of the elements of an $m \\times n$ matrix $A$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[190.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related quantity is the *mean*, which is also called the *average*. \n",
    "We calculate the mean by dividing the sum by the total number of elements. \n",
    "With mathematical notation, we could write the average \n",
    "over a vector $\\mathbf{u}$ as $\\frac{1}{d} \\sum_{i=1}^{d} u_i$ \n",
    "and the average over a matrix $A$ as  $\\frac{1}{n \\cdot m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$. \n",
    "In code, we could just call ``nd.mean()`` on tensors of arbitrary shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9.5]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[9.5]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(nd.mean(A))\n",
    "print(nd.sum(A) / A.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot products\n",
    "\n",
    "<!-- So far, we've only performed element-wise operations, sums and averages. And if this was we could do, linear algebra probably wouldn't deserve it's own chapter. However, -->\n",
    "\n",
    "One of the most fundamental operations is the dot product. Given two vectors $\\mathbf{u}$ and $\\mathbf{v}$, the dot product $\\mathbf{u}^T \\mathbf{v}$ is a sum over the products of the corresponding elements: $\\mathbf{u}^T \\mathbf{v} = \\sum_{i=1}^{d} u_i \\cdot v_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[30.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can express the dot product of two vectors ``nd.dot(u, v)`` equivalently by performing an element-wise multiplication and then a sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[30.]\n",
       "<NDArray 1 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.sum(u * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given a set of weights $\\mathbf{w}$, the weighted sum of some values ${u}$ could be expressed as the dot product $\\mathbf{u}^T \\mathbf{w}$. When the weights are non-negative and sum to one ($\\sum_{i=1}^{d} {w_i} = 1$), the dot product expresses a *weighted average*. When two vectors each have length one (we'll discuss what *length* means below in the section on norms), dot products can also capture the cosine of the angle between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-vector products\n",
    "\n",
    "Now that we know how to calculate dot products we can begin to understand matrix-vector products. Let's start off by visualizing a matrix $A$ and a column vector $\\mathbf{x}$.\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix},\\quad\\mathbf{x}=\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "We can visualize the matrix in terms of its row vectors\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "where each $\\mathbf{a}^T_{i} \\in \\mathbb{R}^{m}$\n",
    "is a row vector representing the $i$-th row of the matrix $A$.\n",
    "\n",
    "Then the matrix vector product $\\mathbf{y} = A\\mathbf{x}$ is simply a column vector $\\mathbf{y} \\in \\mathbb{R}^n$ where each entry $y_i$ is the dot product $\\mathbf{a}^T_i \\mathbf{x}$.\n",
    "\n",
    "$$A\\mathbf{x}=\n",
    "\\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    " \\mathbf{a}^T_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^T_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^T_{n} \\mathbf{x}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So you can think of multiplication by a matrix $A\\in \\mathbb{R}^{m \\times n}$ as a transformation that projects vectors from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$.\n",
    "\n",
    "These transformations turn out to be quite useful. For example, we can represent rotations as multiplications by a square matrix. As we'll see in subsequent chapters, we can also use matrix-vector products to describe the calculations of each layer in a neural network. \n",
    "\n",
    "Expressing matrix-vector products in code with ``ndarray``, we use the same ``nd.dot()`` function as for dot products. When we call ``nd.dot(A, x)`` with a matrix ``A`` and a vector ``x``, ``MXNet`` knows to perform a matrix-vector product. Note that the column dimension of ``A`` must be the same as the dimension of ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 34.  94. 154. 214. 274.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(A, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-matrix multiplication\n",
    "\n",
    "If you've gotten the hang of dot products and matrix-vector multiplication, then matrix-matrix multiplications should be pretty straightforward.\n",
    "\n",
    "Say we have two matrices, $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{pmatrix},\\quad\n",
    "B=\\begin{pmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "To produce the matrix product $C = AB$, it's easiest to think of $A$ in terms of its row vectors and $B$ in terms of its column vectors:\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix},\n",
    "\\quad B=\\begin{pmatrix}\n",
    "\\vdots & \\vdots &  & \\vdots \\\\\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    " \\vdots & \\vdots &  &\\vdots\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Note here that each row vector $\\mathbf{a}^T_{i}$ lies in $\\mathbb{R}^k$ and that each column vector $\\mathbf{b}_j$ also lies in $\\mathbb{R}^k$.\n",
    "\n",
    "Then to produce the matrix product $C \\in \\mathbb{R}^{n \\times m}$ we simply compute each entry $c_{ij}$ as the dot product $\\mathbf{a}^T_i \\mathbf{b}_j$.\n",
    "\n",
    "$$C = AB = \\begin{pmatrix}\n",
    "\\cdots & \\mathbf{a}^T_{1} &...  \\\\\n",
    "\\cdots & \\mathbf{a}^T_{2} & \\cdots \\\\\n",
    " & \\vdots &  \\\\\n",
    " \\cdots &\\mathbf{a}^T_n & \\cdots \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\vdots & \\vdots &  & \\vdots \\\\\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    " \\vdots & \\vdots &  &\\vdots\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\mathbf{b}_1 & \\mathbf{a}^T_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^T_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^T_{2}\\mathbf{b}_1 & \\mathbf{a}^T_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^T_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^T_{n} \\mathbf{b}_1 & \\mathbf{a}^T_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^T_{n} \\mathbf{b}_m \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can think of the matrix-matrix multiplication $AB$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix. Just as with ordinary dot products and matrix-vector products, we can compute matrix-matrix products in ``MXNet`` by using ``nd.dot()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[4. 4. 4. 4. 4.]\n",
       " [4. 4. 4. 4. 4.]\n",
       " [4. 4. 4. 4. 4.]]\n",
       "<NDArray 3x5 @cpu(0)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.ones(shape=(3, 4))\n",
    "B = nd.ones(shape=(4, 5))\n",
    "nd.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate data the MXNet way with `ndarray`\n",
    "\n",
    "It's impossible to get anything done if we can't manipulate data. \n",
    "Generally, there are two important things we need to do with: \n",
    "(i) acquire it! and (ii) process it once it's inside the computer.\n",
    "There's no point in trying to acquire data if we don't even know how to store it,\n",
    "so let's get our hands dirty first by playing with synthetic data.\n",
    "\n",
    "We'll start by introducing NDArrays, MXNet's primary tool for storing and transforming data. If you've worked with NumPy before, you'll notice that NDArrays are, by design, similar to NumPy's multi-dimensional array. However, they confer a few key advantages. First, NDArrays support asynchronous computation on CPU, GPU, and distributed cloud architectures. Second, they provide support for automatic differentiation. These properties make NDArray an ideal library for machine learning, both for researchers and engineers launching production systems.\n",
    "\n",
    "\n",
    "## Getting started\n",
    "\n",
    "In this chapter, we'll get you going with the basic functionality. Don't worry if you don't understand any of the basic math, like element-wise operations or normal distributions. In the next two chapters we'll take another pass at NDArray, teaching you both the math you'll need and how to realize it in code.\n",
    "\n",
    "To get started, let's import `mxnet`. We'll also import `ndarray` from `mxnet` for convenience. We’ll make a habit of setting a random seed so that you always get the same results that we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see how to create an NDArray, without any values initialized. Specifically, we'll create a 2D array (also called a *matrix*) with 3 rows and 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[4.4732473e-35 3.0813152e-41 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "<NDArray 3x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.empty((3, 4))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `empty` method just grabs some memory and hands us back a matrix without setting the values of any of its entries. This means that the entries can have any form of values, including very big ones! But typically, we'll want our matrices initialized. Commonly, we want a matrix of all zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0.]\n",
       " [0. 0. 0. 0. 0.]]\n",
       "<NDArray 3x5 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.zeros((3, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, `ndarray` has a function to create a matrix of all ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 1. 1. 1.]\n",
       " [1. 1. 1. 1.]\n",
       " [1. 1. 1. 1.]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nd.ones((3, 4))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we'll want to create arrays whose values are sampled randomly. This is especially common when we intend to use the array as a parameter in a neural network. In this snippet, we initialize with values drawn from a standard normal distribution with zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.6776515   0.10073948  0.5759544  -0.3469252 ]\n",
       " [-0.22134334 -1.804719   -0.8064291   1.220331  ]\n",
       " [ 2.2323563   0.2007023  -0.5496865  -0.19819015]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nd.random_normal(0, 1, shape=(3, 4))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in NumPy, the dimensions of each NDArray are accessible via the `.shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query its size, which is equal to the product of the components of the shape. Together with the precision of the stored values, this tells us how much memory the array occupies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "NDArray supports a large number of standard mathematical operations. Such as element-wise addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.32234848  1.1007395   1.5759544   0.6530748 ]\n",
       " [ 0.77865666 -0.804719    0.19357091  2.220331  ]\n",
       " [ 3.2323563   1.2007023   0.4503135   0.80180985]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.6776515   0.10073948  0.5759544  -0.3469252 ]\n",
       " [-0.22134334 -1.804719   -0.8064291   1.220331  ]\n",
       " [ 2.2323563   0.2007023  -0.5496865  -0.19819015]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And exponentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.50780815 1.1059885  1.7788274  0.7068582 ]\n",
       " [0.8014415  0.16452068 0.44644946 3.388309  ]\n",
       " [9.321805   1.2222608  0.57713073 0.82021385]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.exp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also grab a matrix's transpose to compute a proper matrix-matrix product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.34788287 -1.6121604   1.6851819 ]\n",
       " [-0.34788287 -1.6121604   1.6851819 ]\n",
       " [-0.34788287 -1.6121604   1.6851819 ]]\n",
       "<NDArray 3x3 @cpu(0)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(x, y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations\n",
    "\n",
    "In the previous example, every time we ran an operation, we allocated new memory to host its results. For example, if we write `y = x + y`, we will dereference the matrix that `y` used to point to and instead point it at the newly allocated memory. In the following example we demonstrate this with Python's `id()` function, which gives us the exact address of the referenced object in memory. After running `y = y + x`, we'll find that `id(y)` points to a different location. That's because Python first evaluates `y + x`, allocating new memory for the result and then subsequently redirects `y` to point at this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(y): 140366177499728\n",
      "id(y): 140366177515856\n"
     ]
    }
   ],
   "source": [
    "print('id(y):', id(y))\n",
    "y = y + x\n",
    "print('id(y):', id(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be undesirable for two reasons. First, we don't want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of paramaters and update all of them multiple times per second. Typically, we'll want to perform these updates in place. Second, we might point at the same parameters from multiple variables. If we don't update in place, this could cause a memory leak, and could cause us to inadvertently reference stale parameters. \n",
    "\n",
    "Fortunately, performing in-place operations in MXNet is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., `y[:] = <expression>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(y): 140366177515856\n",
      "id(y): 140366177515856\n"
     ]
    }
   ],
   "source": [
    "print('id(y):', id(y))\n",
    "y[:] = x + y\n",
    "print('id(y):', id(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this syntacically nice, `x+y` here will still allocate a temporary buffer to store the result before copying it to `y[:]`. To make even better use of memory, we can directly invoke the underlying `ndarray` operation, in this case `elemwise_add`, avoiding temporary buffers. We do this by specifying the `out` keyword argument, which every `ndarray` operator supports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[2.3223486 3.1007395 3.5759544 2.6530747]\n",
       " [2.7786567 1.195281  2.1935709 4.220331 ]\n",
       " [5.232356  3.2007022 2.4503136 2.8018098]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.elemwise_add(x, y, out=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're not planning to re-use ``x``, then we can assign the result to ``x`` itself. There are two ways to do this in MXNet. \n",
    "1. By using slice notation x[:] = x op y\n",
    "2. By using the op-equals operators like `+=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(x): 140366177515216\n",
      "id(x): 140366177515216\n"
     ]
    }
   ],
   "source": [
    "print('id(x):', id(x))\n",
    "x += y\n",
    "x\n",
    "print('id(x):', id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing\n",
    "MXNet NDArrays support slicing in all the ridiculous ways you might imagine accessing your data. Here's an example of reading the second and third rows from `x`.mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[3.7786567 2.195281  3.1935709 5.220331 ]\n",
       " [6.232356  4.200702  3.4503136 3.8018098]]\n",
       "<NDArray 2x4 @cpu(0)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try writing to a specific element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[3.3223486 4.1007395 4.5759544 3.6530747]\n",
       " [3.7786567 2.195281  9.        5.220331 ]\n",
       " [6.232356  4.200702  3.4503136 3.8018098]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,2] = 9.0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-dimensional slicing is also supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[2.195281 9.      ]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:2,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[3.3223486 4.1007395 4.5759544 3.6530747]\n",
       " [3.7786567 5.        5.        5.220331 ]\n",
       " [6.232356  4.200702  3.4503136 3.8018098]]\n",
       "<NDArray 3x4 @cpu(0)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:2,1:3] = 5.0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "You might wonder, what happens if you add a vector `y` to a matrix `X`? These operations, where we compose a low dimensional array `y` with a high-dimensional array `X` invoke a functionality called broadcasting. Here, the low-dimensional array is duplicated along any axis with dimension $1$ to match the shape of the high dimensional array. Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  \n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "<NDArray 3x3 @cpu(0)>\n",
      "y =  \n",
      "[0. 1. 2.]\n",
      "<NDArray 3 @cpu(0)>\n",
      "x + y =  \n",
      "[[1. 2. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 2. 3.]]\n",
      "<NDArray 3x3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "x = nd.ones(shape=(3,3))\n",
    "print('x = ', x)\n",
    "y = nd.arange(3)\n",
    "print('y = ', y)\n",
    "print('x + y = ', x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `y` is initially of shape (3), \n",
    "MXNet infers its shape to be (1,3), \n",
    "and then broadcasts along the rows to form a (3,3) matrix). \n",
    "You might wonder, why did MXNet choose to interpret `y` as a (1,3) matrix and not (3,1). \n",
    "That's because broadcasting prefers to duplicate along the left most axis. \n",
    "We can alter this behavior by explicitly giving `y` a 2D shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y =  \n",
      "[[0.]\n",
      " [1.]\n",
      " [2.]]\n",
      "<NDArray 3x1 @cpu(0)>\n",
      "x + y =  \n",
      "[[1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]]\n",
      "<NDArray 3x3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "y = y.reshape((3,1))\n",
    "print('y = ', y)\n",
    "print('x + y = ', x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting from MXNet NDArray to NumPy\n",
    "Converting MXNet NDArrays to and from NumPy is easy. The converted arrays do not share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = x.asnumpy()\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 1. 1.]\n",
       " [1. 1. 1.]\n",
       " [1. 1. 1.]]\n",
       "<NDArray 3x3 @cpu(0)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = nd.array(a) \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation with ``autograd`` \n",
    "\n",
    "\n",
    "In machine learning, we *train* models to get better and better as a function of experience. Usually, *getting better* means minimizing a *loss function*, i.e. a score that answers \"how *bad* is our model?\" With neural networks, we choose loss functions to be differentiable with respect to our parameters. Put simply, this means that for each of the model's parameters, we can determine how much *increasing* or *decreasing* it might affect the loss. While the calculations are straightforward, for complex models, working it out by hand can be a pain.\n",
    "\n",
    "_MXNet_'s autograd package expedites this work by automatically calculating derivatives. And while most other libraries require that we compile a symbolic graph to take automatic derivatives, ``mxnet.autograd``, like PyTorch, allows you to take derivatives while writing  ordinary imperative code. Every time you make pass through your model, ``autograd`` builds a graph on the fly, through which it can immediately backpropagate gradients.\n",
    "\n",
    "Let's go through it step by step. For this tutorial, we'll only need to import ``mxnet.ndarray``, and ``mxnet.autograd``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching gradients\n",
    "\n",
    "As a toy example, Let's say that we are interested in differentiating a function ``f = 2 * (x ** 2)`` with respect to parameter x. We can start by assigning an initial value of ``x``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we compute the gradient of ``f`` with respect to ``x``, we'll need a place to store it. In _MXNet_, we can tell an NDArray that we plan to store a gradient by invoking its ``attach_grad()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to define the function ``f`` and *MXNet* will generate a computation graph on the fly. It's as if *MXNet* turned on a recording device and captured the exact path by which each variable was generated. \n",
    "\n",
    "Note that building the computation graph requires a nontrivial amount of computation. So *MXNet* will only build the graph when explicitly told to do so. We can instruct *MXNet* to start recording by placing code inside a ``with autograd.record():`` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = x * 2\n",
    "    z = y * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's backprop by calling ``z.backward()``. When ``z`` has more than one entry, ``z.backward()`` is equivalent to mx.nd.sum(z).backward().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if this is the expected output. Remember that ``y = x * 2``, and ``z = x * y``, so ``z`` should be equal to  ``2 * x * x``. After, doing backprop with ``z.backward()``, we expect to get back gradient dz/dx as follows: dy/dx = ``2``, dz/dx = ``4 * x``. So, if everything went according to plan, ``x.grad`` should consist of an NDArray with the values ``[[4, 8],[12, 16]]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 4.  8.]\n",
      " [12. 16.]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head gradients and the chain rule\n",
    "\n",
    "*Caution: This part is tricky, but not necessary to understanding subsequent sections.*\n",
    "\n",
    "Sometimes when we call the backward method on an NDArray, e.g. ``y.backward()``, where ``y`` is a function of ``x`` we are just interested in the derivative of ``y`` with respect to ``x``. Mathematicians write this as $\\frac{dy(x)}{dx}$. At other times, we may be interested in the gradient of ``z`` with respect to ``x``, where ``z`` is a function of ``y``, which in turn, is a function of ``x``. That is, we are interested in $\\frac{d}{dx} z(y(x))$. Recall that by the chain rule $\\frac{d}{dx} z(y(x)) = \\frac{dz(y)}{dy} \\frac{dy(x)}{dx}$. So, when ``y`` is part of a larger function ``z``, and we want ``x.grad`` to store $\\frac{dz}{dx}$, we can pass in the *head gradient* $\\frac{dz}{dy}$ as an input to ``backward()``. The default argument is ``nd.ones_like(y)``. See [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[40.    8.  ]\n",
      " [ 1.2   0.16]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with autograd.record():\n",
    "    y = x * 2\n",
    "    z = y * x\n",
    "\n",
    "head_gradient = nd.array([[10, 1.], [.1, .01]])\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the basics, we can do some wild things with autograd, including building differentiable functions using Pythonic control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.random_normal(shape=3)\n",
    "a.attach_grad()\n",
    "\n",
    "with autograd.record():\n",
    "    b = a * 2\n",
    "    while (nd.norm(b) < 1000).asscalar():\n",
    "        b = b * 2\n",
    "\n",
    "    if (mx.nd.sum(b) > 0).asscalar():\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_gradient = nd.array([0.01, 1.0, .1])\n",
    "c.backward(head_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[  2048. 204800.  20480.]\n",
      "<NDArray 3 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with ``gluon``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the context\n",
    "\n",
    "We'll also want to set a context to tell gluon where to do most of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset\n",
    "\n",
    "Again we'll look at the problem of linear regression and stick with the same synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 10000\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "    \n",
    "X = nd.random_normal(shape=(num_examples, num_inputs))\n",
    "noise = 0.01 * nd.random_normal(shape=(num_examples,))\n",
    "y = real_fn(X) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data iterator\n",
    "\n",
    "We'll stick with the ``DataLoader`` for handling our data batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(X, y),\n",
    "                                      batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "When we implemented things from scratch, \n",
    "we had to individually allocate parameters \n",
    "and then compose them together as a model. \n",
    "While it's good to know how to do things from scratch, \n",
    "with `gluon`, we can just compose a network from predefined layers. \n",
    "For a linear model, the appropriate layer is called `Dense`. \n",
    "It's called a *dense* layer because every node in the input \n",
    "is connected to every node in the subsequent layer. \n",
    "That description seems excessive \n",
    "because we only have one (non-input) layer here, \n",
    "and that layer only contains one node!\n",
    "But in subsequent chapters we'll typically work \n",
    "with networks that have multiple outputs, \n",
    "so we might as well start thinking in terms of layers of nodes. \n",
    "Because a linear model consists of just a single `Dense` layer, we can instantiate it with one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Dense(1, in_units=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We've already got a neural network. \n",
    "Like our hand-crafted model in the previous notebook, \n",
    "this model has a weight matrix and bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dense0_weight (shape=(1, 2), dtype=float32)\n",
      "Parameter dense0_bias (shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(net.weight)\n",
    "print(net.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `net.weight` and `net.bias` are not actually NDArrays.\n",
    "They are instances of the `Parameter` class.\n",
    "We use `Parameter` instead of directly accessing NDAarrays for several reasons. \n",
    "For example, they provide convenient abstractions for initializing values.\n",
    "Unlike NDArrays, Parameters can be associated with multiple contexts simultaneously.\n",
    "This will come in handy in future chapters when we start thinking about distributed learning across multiple GPUs.\n",
    "\n",
    "In `gluon`, all neural networks are made out of Blocks (`gluon.Block`).\n",
    "Blocks are just units that take inputs and generate outputs.\n",
    "Blocks also contain parameters that we can update. \n",
    "Here, our network consists of only one layer, \n",
    "so it's convenient to access our parameters directly. \n",
    "When our networks consist of 10s of layers, this won't be so fun.\n",
    "No matter how complex our network, \n",
    "we can grab all its parameters by calling `collect_params()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dense0_ (\n",
       "  Parameter dense0_weight (shape=(1, 2), dtype=float32)\n",
       "  Parameter dense0_bias (shape=(1,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object is a `gluon.parameter.ParameterDict`. \n",
    "This is a convenient abstraction for retrieving and manipulating groups of Parameter objects.\n",
    "Most often, we'll want to retrieve all of the parameters in a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mxnet.gluon.parameter.ParameterDict"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters\n",
    "Once we initialize our Parameters, we can access their underlying data and context(s),\n",
    "and we can also feed data through the neural network to generate output.\n",
    "However, we can't get going just yet. \n",
    "If we try invoking your model by calling ``net(nd.array([[0,1]]))``, \n",
    "we'll confront the following hideous error message:\n",
    "\n",
    "```RuntimeError: Parameter dense1_weight has not been initialized...```\n",
    "\n",
    "That's because we haven't yet told ``gluon`` what the *initial values* for our parameters should be!\n",
    "We initialize parameters by calling the `.initialize()` method of a ParameterDict. \n",
    "We'll need to pass in two arguments. \n",
    "\n",
    "* An initializer, many of which live in the `mx.init` module. \n",
    "* A context where the parameters should live. In this case we'll pass in the `model_ctx`. Most often this will either be a GPU or a list of GPUs. \n",
    "\n",
    "*MXNet* provides a variety of common initializers in ``mxnet.init``.\n",
    "To keep things consistent with the model we built by hand, \n",
    "we'll initialize each parameter by sampling from a standard normal distribution, \n",
    "using `mx.init.Normal(sigma=1.)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deferred Initialization\n",
    "\n",
    "When we call ``initialize``, ``gluon`` associates each parameter with an initializer.\n",
    "However, the *actual initialization* is deferred until we make a first forward pass. \n",
    "In other words, the parameters are only initialized when they're needed. \n",
    "If we try to call `net.weight.data()` we'll get the following error:\n",
    "\n",
    "``DeferredInitializationError: Parameter dense2_weight has not been initialized yet because initialization was deferred. Actual initialization happens during the first forward pass. Please pass one batch of data through the network before accessing Parameters.``\n",
    "\n",
    "Passing data through a `gluon` model is easy. \n",
    "We just sample a batch of the appropriate shape and call `net` \n",
    "just as if it were a function. \n",
    "This will invoke `net`'s `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[9.815207]]\n",
       "<NDArray 1x1 @cpu(0)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data = nd.array([[4,7]])\n",
    "net(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that `net` is initialized, we can access each of its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.4016273 1.1726711]]\n",
      "<NDArray 1x2 @cpu(0)>\n",
      "\n",
      "[0.]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(net.weight.data())\n",
    "print(net.bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape inference\n",
    "\n",
    "Recall that previously, we instantiated our network with `gluon.nn.Dense(1, in_units=2)`. \n",
    "One slick feature that we can take advantage of in ``gluon`` is shape inference on parameters. \n",
    "Because our parameters never come into action until we pass data through the network,\n",
    "we don't actually have to declare the input dimension (`in_units`). \n",
    "Let's try this again, but letting `gluon` do more of the work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Dense(1)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss\n",
    "\n",
    "Instead of writing our own loss function we're just going to access squared error by instantiating ``gluon.loss.L2Loss``. Just like layers, and whole networks, a loss in gluon is just a `Block`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "Instead of writing stochastic gradient descent from scratch every time, we can instantiate a ``gluon.Trainer``, passing it a dictionary of parameters. Note that the ``SGD`` optimizer in ``gluon`` also has a few bells and whistles that you can turn on at will, including *momentum* and *clipping* (both are switched off by default). These modifications can help to converge faster and we'll discuss them later when we go over a variety of optimization algorithms in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.0001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training loop\n",
    "\n",
    "You might have noticed that it was a bit more concise to express our model in ``gluon``. For example, we didn't have to individually allocate parameters, define our loss function, or implement stochastic gradient descent. The benefits of relying on ``gluon``'s abstractions will grow substantially once we start working with much more complex models. But once we have all the basic pieces in place, the training loop itself is quite similar to what we would do if implementing everything from scratch. \n",
    "\n",
    "To refresh your memory. For some number of ``epochs``, we'll make a complete pass over the dataset (``train_data``), grabbing one mini-batch of inputs and the corresponding ground-truth labels at a time. \n",
    "\n",
    "Then, for each batch, we'll go through the following ritual. So that this process becomes maximally ritualistic, we'll repeat it verbatim:\n",
    "\n",
    "* Generate predictions (``yhat``) and the loss (``loss``) by executing a forward pass through the network.\n",
    "* Calculate gradients by making a backwards pass through the network via ``loss.backward()``. \n",
    "* Update the model parameters by invoking our SGD optimizer (note that we need not tell ``trainer.step`` about which parameters but rather just the amount of data, since we already performed that in the initialization of ``trainer``).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 3.130042259299755\n",
      "Epoch 1, loss: 1.9178876393541693\n",
      "Epoch 2, loss: 1.1751932098899036\n",
      "Epoch 3, loss: 0.7201292191464453\n",
      "Epoch 4, loss: 0.44129439100138845\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "loss_sequence = []\n",
    "num_batches = num_examples / batch_size\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    # inner loop\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_loss += nd.mean(loss).asscalar()\n",
    "    print(\"Epoch %s, loss: %s\" % (e, cumulative_loss / num_examples))\n",
    "    loss_sequence.append(cumulative_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.1-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 2.9 MB/s eta 0:00:01    |██████▍                         | 2.3 MB 2.9 MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from matplotlib) (2020.6.20)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from matplotlib) (1.19.1)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
      "Requirement already satisfied: six>=1.5 in /home/paul/anaconda3/envs/mxnet/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Installing collected packages: kiwisolver, cycler, pillow, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.1 pillow-7.2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'average loss')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "# plot the convergence of the estimated loss function \n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(num=None,figsize=(8, 6))\n",
    "plt.plot(loss_sequence)\n",
    "\n",
    "# Adding some bells and whistles to the plot\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.xlabel('epoch',fontsize=14)\n",
    "plt.ylabel('average loss',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the loss function converges quickly to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the learned model parameters\n",
    "\n",
    "As an additional sanity check, since we generated the data from a Gaussian linear regression model, we want to make sure that the learner managed to recover the model parameters, which were set to weight $2,-3.4$ with an offset of $4.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of \"params\" is a  <class 'mxnet.gluon.parameter.ParameterDict'>\n",
      "dense1_weight \n",
      "[[ 1.2376138 -2.548026 ]]\n",
      "<NDArray 1x2 @cpu(0)>\n",
      "dense1_bias \n",
      "[2.9809098]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params() # this returns a ParameterDict\n",
    "\n",
    "print('The type of \"params\" is a ',type(params))\n",
    "\n",
    "# A ParameterDict is a dictionary of Parameter class objects\n",
    "# therefore, here is how we can read off the parameters from it.\n",
    "\n",
    "for param in params.values():\n",
    "    print(param.name,param.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks in ``gluon``\n",
    "\n",
    "Now let's see how succinctly we can express a convolutional neural network using ``gluon``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/paul/.mxnet/datasets/mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\n",
      "Downloading /home/paul/.mxnet/datasets/mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\n",
      "Downloading /home/paul/.mxnet/datasets/mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n",
      "Downloading /home/paul/.mxnet/datasets/mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a convolutional neural network\n",
    "\n",
    "Again, a few lines here is all we need in order to change the model. Let's add a couple of convolutional layers using ``gluon.nn``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fc = 512\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))            \n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(num_fc, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax cross-entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write evaluation loop to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.07862706502291344, Train_acc 0.9814833333333334, Test_acc 0.9817\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "        \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very deep networks with repeating elements\n",
    "\n",
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The VGG architecture\n",
    "\n",
    "A key aspect of VGG was to use many convolutional blocks with relatively narrow kernels, followed by a max-pooling step and to repeat this block multiple times. What is pretty neat about the code below is that we use functions to *return* network blocks. These are then combined to larger networks (e.g. in `vgg_stack`) and this allows us to construct VGG from components. What is particularly useful here is that we can use it to reparameterize the architecture simply by changing a few lines rather than adding and removing many lines of network definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "def vgg_block(num_convs, channels):\n",
    "    out = nn.Sequential()\n",
    "    for _ in range(num_convs):\n",
    "        out.add(nn.Conv2D(channels=channels, kernel_size=3,\n",
    "                      padding=1, activation='relu'))\n",
    "    out.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    return out\n",
    "\n",
    "def vgg_stack(architecture):\n",
    "    out = nn.Sequential()\n",
    "    for (num_convs, channels) in architecture:\n",
    "        out.add(vgg_block(num_convs, channels))\n",
    "    return out\n",
    "\n",
    "num_outputs = 10\n",
    "architecture = ((1,64), (1,128), (2,256), (2,512))\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(vgg_stack(architecture))\n",
    "    net.add(nn.Flatten())\n",
    "    net.add(nn.Dense(512, activation=\"relu\"))\n",
    "    net.add(nn.Dropout(.5))\n",
    "    net.add(nn.Dense(512, activation=\"relu\"))\n",
    "    net.add(nn.Dropout(.5))\n",
    "    net.add(nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .05})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for d, l in data_iterator:\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = l.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200. Loss: 2.298513\n",
      "Batch 400. Loss: 2.237960\n",
      "Batch 600. Loss: 1.059641\n",
      "Batch 800. Loss: 0.415175\n",
      "Epoch 0. Loss: 0.2514230734380071, Train_acc 0.9439, Test_acc 0.9458\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "#  Only one epoch so tests can run quickly, increase this variable to actually run\n",
    "###########################\n",
    "epochs = 1\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (d, l) in enumerate(train_data):\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = l.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "        \n",
    "        if i > 0 and i % 200 == 0:\n",
    "            print('Batch %d. Loss: %f' % (i, moving_loss))\n",
    "            \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
